{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmark_dict = {\n",
    "    0: \"nose\",\n",
    "    1: \"left_eye_inner\",\n",
    "    2: \"left_eye\",\n",
    "    3: \"left_eye_outer\",\n",
    "    4: \"right_eye_inner\",\n",
    "    5: \"right_eye\",\n",
    "    6: \"right_eye_outer\",\n",
    "    7: \"left_ear\",\n",
    "    8: \"right_ear\",\n",
    "    9: \"mouth_left\",\n",
    "    10: \"mouth_right\",\n",
    "    11: \"left_shoulder\",\n",
    "    12: \"right_shoulder\",\n",
    "    13: \"left_elbow\",\n",
    "    14: \"right_elbow\",\n",
    "    15: \"left_wrist\",\n",
    "    16: \"right_wrist\",\n",
    "    17: \"left_pinky\",\n",
    "    18: \"right_pinky\",\n",
    "    19: \"left_index\",\n",
    "    20: \"right_index\",\n",
    "    21: \"left_thumb\",\n",
    "    22: \"right_thumb\",\n",
    "    23: \"left_hip\",\n",
    "    24: \"right_hip\",\n",
    "    25: \"left_knee\",\n",
    "    26: \"right_knee\",\n",
    "    27: \"left_ankle\",\n",
    "    28: \"right_ankle\",\n",
    "    29: \"left_heel\",\n",
    "    30: \"right_heel\",\n",
    "    31: \"left_foot_index\",\n",
    "    32: \"right_foot_index\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_data(file_name):\n",
    "  mp_drawing = mp.solutions.drawing_utils\n",
    "  mp_drawing_styles = mp.solutions.drawing_styles\n",
    "  mp_holistic = mp.solutions.holistic\n",
    "\n",
    "  # VID_DIM = (1290, 1080)\n",
    "  counter = 0\n",
    "  cap = cv2.VideoCapture(f\"C:/Users/chengyang/Physio/videos/{file_name}\")\n",
    "  # used to record the time when we processed last frame \n",
    "  prev_frame_time = 0\n",
    "    \n",
    "  # used to record the time at which we processed current frame \n",
    "  new_frame_time = 0\n",
    "  stage = None\n",
    "\n",
    "  frame_width = int(cap.get(3))\n",
    "  frame_height = int(cap.get(4))\n",
    "  frame_size = (frame_width,frame_height)\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "  file = cv2.VideoWriter('output2.mp4', fourcc, 20.0, frame_size)\n",
    "  data = []\n",
    "\n",
    "  file_name = file_name.split(\".\")[0]\n",
    "  landmarks = ['class']\n",
    "  for val in range(1, 33+1):\n",
    "      landmarks += ['x{}'.format(val), 'y{}'.format(val), 'z{}'.format(val), 'v{}'.format(val)]\n",
    "\n",
    "  with open(f\"C:/Users/chengyang/Physio/src/PoseDetection/SquatData/{file_name}.csv\", mode='w', newline='') as f:\n",
    "      csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "      csv_writer.writerow(landmarks)\n",
    "\n",
    "  with mp_holistic.Holistic(\n",
    "      model_complexity = 0,\n",
    "      min_detection_confidence=0.5,\n",
    "      min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "      success, image = cap.read()\n",
    "      if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        break\n",
    "\n",
    "      new_frame_time = time.time() \n",
    "    \n",
    "      # Calculating the fps \n",
    "    \n",
    "      # fps will be number of frame processed in given time frame \n",
    "      # since their will be most of time error of 0.001 second \n",
    "      # we will be subtracting it to get more accurate result \n",
    "      fps = 1/(new_frame_time-prev_frame_time) \n",
    "      prev_frame_time = new_frame_time \n",
    "\n",
    "      image = cv2.flip(image, 1)\n",
    "      \n",
    "      # To improve performance, optionally mark the image as not writeable to\n",
    "      # pass by reference.\n",
    "      image.flags.writeable = False\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      results = holistic.process(image)\n",
    "\n",
    "      # Draw the hand annotations on the image.\n",
    "      image.flags.writeable = True\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "      mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing_styles.get_default_hand_landmarks_style(), mp_drawing_styles.get_default_hand_connections_style())\n",
    "      mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, mp_drawing_styles.get_default_hand_landmarks_style(), mp_drawing_styles.get_default_hand_connections_style())\n",
    "      mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "\n",
    "      try:\n",
    "          landmarks = results.pose_world_landmarks.landmark\n",
    "          # Get coordinates\n",
    "          # Define a list of landmark names\n",
    "          # landmark_names = [mp_holistic.PoseLandmark(i).name for i in range(mp_holistic.PoseLandmark.NUM_POSE_LANDMARKS)]\n",
    "\n",
    "          # Create a dictionary to store landmark coordinates\n",
    "          # coordinates = {}\n",
    "\n",
    "          # for id, pose_landmarks in enumerate(landmarks):\n",
    "          #     print(pose_landmarks)\n",
    "          #     # Here is How to Get All the Coordinates\n",
    "          #     cx, cy, cz = pose_landmarks.x * frame_width, pose_landmarks.y * frame_height, pose_landmarks.z * frame_height\n",
    "          #     coordinates[landmark_dict[id]] = [cx, cy, cz]\n",
    "          # coordinates[\"exercise\"] = \"squat\"\n",
    "          # data.append(coordinates)\n",
    "\n",
    "          pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in landmarks]).flatten())        \n",
    "          \n",
    "          # Export to CSV\n",
    "          with open(f\"C:/Users/chengyang/Physio/src/PoseDetection/SquatData/{file_name}.csv\", mode='a', newline='') as f:\n",
    "              csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "              csv_writer.writerow(pose_row) \n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "          pass\n",
    "      \n",
    "      # Render curl counter\n",
    "      # Setup status box\n",
    "      cv2.rectangle(image, (0,0), (325,73), (245,117,16), -1)\n",
    "      \n",
    "      # Rep data\n",
    "      cv2.putText(image, 'REPS', (15,12), \n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "      cv2.putText(image, str(counter), \n",
    "                  (10,60), \n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "      \n",
    "      # Stage data\n",
    "      cv2.putText(image, 'STAGE', (115,12), \n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "      cv2.putText(image, stage, \n",
    "                  (100,60), \n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "      \n",
    "      cv2.putText(image, 'FPS: ' + str(int(fps)), (225,12), \n",
    "                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "      # Write the frame to the video file\n",
    "      file.write(image)\n",
    "\n",
    "      # Flip the image horizontally for a selfie-view display.\n",
    "      cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "      if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "              break\n",
    "  cap.release()\n",
    "  file.release()\n",
    "  cv2.destroyAllWindows()\n",
    "  # data = pd.DataFrame(data)\n",
    "  # file_name = file_name.split(\".\")[0]\n",
    "  # data.to_csv(f\"C:/Users/chengyang/Physio/src/PoseDetection/SquatData/{file_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "Ignoring empty camera frame.\n",
      "20231116_231303.mp4\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "Ignoring empty camera frame.\n",
      "20231116_231348.mp4\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "Ignoring empty camera frame.\n",
      "20231116_231413.mp4\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "Ignoring empty camera frame.\n",
      "20231116_231434.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat1.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat10.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat2.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat3.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat4.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat5.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat6.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat7.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat8.mp4\n",
      "Ignoring empty camera frame.\n",
      "notEnoughBendSquat9.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat1.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat2.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat3.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat4.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat5.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat6.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat7.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat8.mp4\n",
      "Ignoring empty camera frame.\n",
      "squat9.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"C:/Users/chengyang/Physio/videos/\"  # Replace with the path to your folder\n",
    "\n",
    "# List all files in the folder\n",
    "files = os.listdir(folder_path)\n",
    "\n",
    "# Loop through the files\n",
    "for file_name in files:\n",
    "    # Check if the item is a file (not a subdirectory)\n",
    "    if os.path.isfile(os.path.join(folder_path, file_name)):\n",
    "        extract_data(file_name)\n",
    "        print(file_name)\n",
    "        # Add your processing logic for each file here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label the datasets\n",
    "## column for type of exercise, \n",
    "## maybe should just collect all the body landmarks, \n",
    "## column for feedback (good, bend the knees more, straighten back, feet too wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a Graph convolutional layer with a learnable adjacency matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, node_n=57):\n",
    "\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        self.adj = Parameter(torch.FloatTensor(node_n, node_n))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.adj.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input):\n",
    "        support = torch.matmul(input, self.weight)\n",
    "        output = torch.matmul(self.adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GC_Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Define a residual block of GCN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, p_dropout, bias=True, node_n=57):\n",
    "\n",
    "        super(GC_Block, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = in_features\n",
    "\n",
    "        self.gc1 = GraphConvolution(in_features, in_features, node_n=node_n, bias=bias)\n",
    "        self.bn1 = nn.BatchNorm1d(node_n * in_features)\n",
    "\n",
    "        self.gc2 = GraphConvolution(in_features, in_features, node_n=node_n, bias=bias)\n",
    "        self.bn2 = nn.BatchNorm1d(node_n * in_features)\n",
    "\n",
    "        self.do = nn.Dropout(p_dropout)\n",
    "        self.act_f = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.gc1(x)\n",
    "        if len(y.shape) == 3:\n",
    "            b, n, f = y.shape\n",
    "        else:\n",
    "            b = 1\n",
    "            n, f = y.shape\n",
    "        y = self.bn1(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        y = self.gc2(y)\n",
    "        y = self.bn2(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        return y + x\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GCN_corr(nn.Module):\n",
    "\n",
    "    def __init__(self, input_feature=25, hidden_feature=128, p_dropout=0.5, num_stage=2, node_n=57):\n",
    "        \"\"\"\n",
    "        :param input_feature: num of input feature\n",
    "        :param hidden_feature: num of hidden feature\n",
    "        :param p_dropout: drop out prob.\n",
    "        :param num_stage: number of residual blocks\n",
    "        :param node_n: number of nodes in graph\n",
    "        \"\"\"\n",
    "        super(GCN_corr, self).__init__()\n",
    "        self.num_stage = num_stage\n",
    "\n",
    "        self.gcin = GraphConvolution(input_feature, hidden_feature, node_n=node_n)\n",
    "        self.bn1 = nn.BatchNorm1d(node_n * hidden_feature)\n",
    "\n",
    "        self.gcbs = []\n",
    "        for i in range(num_stage):\n",
    "            self.gcbs.append(GC_Block(hidden_feature, p_dropout=p_dropout, node_n=node_n))\n",
    "\n",
    "        self.gcbs = nn.ModuleList(self.gcbs)\n",
    "\n",
    "        self.gcout = GraphConvolution(hidden_feature, input_feature, node_n=node_n)\n",
    "        self.gcatt = GraphConvolution(hidden_feature, 1, node_n=node_n)\n",
    "\n",
    "        self.do = nn.Dropout(p_dropout)\n",
    "        self.act_f = nn.ReLU()\n",
    "        self.act_fatt = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.gcin(x)\n",
    "        if len(y.shape) == 3:\n",
    "            b, n, f = y.shape\n",
    "        else:\n",
    "            b = 1\n",
    "            n, f = y.shape\n",
    "\n",
    "        y = self.bn1(y.view(b, -1)).view(b, n, f)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        for i in range(self.num_stage):\n",
    "            y = self.gcbs[i](y)\n",
    "\n",
    "        out = self.gcout(y)\n",
    "\n",
    "        att = self.gcatt(y)\n",
    "        att = self.act_fatt(att)\n",
    "\n",
    "        return out, att\n",
    "\n",
    "\n",
    "class GCN_class(nn.Module):\n",
    "\n",
    "    def __init__(self, input_feature=25, hidden_feature=32, p_dropout=0.5, node_n=57, classes=12):\n",
    "        \"\"\"\n",
    "        :param input_feature: num of input feature\n",
    "        :param hidden_feature: num of hidden feature\n",
    "        :param p_dropout: drop out prob.\n",
    "        :param num_stage: number of residual blocks\n",
    "        :param node_n: number of nodes in graph\n",
    "        \"\"\"\n",
    "        super(GCN_class, self).__init__()\n",
    "\n",
    "        self.gcin = GraphConvolution(input_feature, hidden_feature, node_n=node_n)\n",
    "        self.gcout = GraphConvolution(hidden_feature, input_feature, node_n=node_n)\n",
    "        self.bnin = nn.BatchNorm1d(node_n * hidden_feature)\n",
    "        self.bnout = nn.BatchNorm1d(node_n * input_feature)\n",
    "        self.lin = nn.Linear(node_n * input_feature, classes)\n",
    "        self.do = nn.Dropout(p_dropout)\n",
    "        self.act_f = nn.ReLU()\n",
    "        self.act_flin = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.shape) == 3:\n",
    "            b, n, f = x.shape\n",
    "        else:\n",
    "            b = 1\n",
    "            n, f = x.shape\n",
    "\n",
    "        y = self.gcin(x)\n",
    "        if b > 1:\n",
    "            y = self.bnin(y.view(b, -1)).view(y.shape)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        y = self.gcout(y)\n",
    "        if b > 1:\n",
    "            y = self.bnout(y.view(b, -1)).view(y.shape)\n",
    "        y = self.act_f(y)\n",
    "        y = self.do(y)\n",
    "\n",
    "        y = y.view(-1, n * f)\n",
    "        y = self.lin(y)\n",
    "        y = self.act_flin(y)\n",
    "\n",
    "        return y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
